{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Gridtools","text":"<p>A high-level API to handle the different types of data recorded with, and extracted from electrode grid recordings of wave-type weakly electric fish.</p>"},{"location":"#installation","title":"Installation","text":"<p>To install the package, clone the repository and simply install it with pip:</p> <pre><code>git clone https://github.com/weygoldt/gridtools.git\ncd gridtools &amp;&amp; pip install .\n</code></pre>"},{"location":"#usage","title":"Usage","text":""},{"location":"#loading-manipulating-and-saving-data","title":"Loading, manipulating and saving data","text":"<p><code>gridtools</code> can easily load <code>wavetracker</code> datasets including  position estimates, communication signals, and the raw recordings  into a single data model that verifies the dataset upon instantiation and automatically loads everything that is presend in the directory. The data model can be manipulated and saved to disk.</p> <p>As an example loading a dataset, creating a subset, and saving it to disk is as easy as:</p> <pre><code>from gridtools.datasets import load, subset, save\ndata = load('path/to/dataset')\ndata = subset(data, 0, 1000) # get the first 1000 seconds\nsave(data, 'path/to/dataset_subset')\n</code></pre> <p>Most of this functionality is also implemented as shell scipts and can  be used from the command line. For example, to create a subset of a dataset, you can run:</p> <pre><code>subset-dataset -i /path/to/dataset -o /path/to/save --start 10 --end 20\n</code></pre> <p>Because the <code>load</code> function automatically loads as much data as possible (excluding the raw data) and verifies the dataset, it is a good idea to check what is included in the dataset before loading it. This can be done with the <code>pprint</code> function. It will print a summary of the dataset.</p> <p><pre><code>data.pprint()\n</code></pre> To also load the raw data as a <code>thunderfish.dataloader.DataLoader</code> object, you can set the <code>grid</code> flag to <code>True</code>:</p> <pre><code>data = load('path/to/dataset', grid=True)\n</code></pre>"},{"location":"#building-preprocessing-pipelines","title":"Building preprocessing pipelines","text":"<p>The first step in electrode grid preprocessing is the <code>wavetracker</code>, that  performs ridge detection on sum spectrograms across all electrodes of the grid to estimate the evolution of each fish's baseline EODf over time. </p> <p><code>gridtools</code> provides a high-level API to build preprocessing pipelines that can be used to filter the data generated by the <code>wavetracker</code>. Common preprocessing steps include:</p> <ul> <li><code>gridtools.preprocessing.remove_unassigned_tracks</code> to remove tracks that were not assigned to a fish identity by the <code>wavetracker</code></li> <li><code>gridtools.preprocessing.remove_short_tracks</code> to remove tracks that are shorter than a given threshold</li> <li><code>gridtools.preprocessing.remove_low_power_tracks</code> to remove tracks that have a low maximum power, i.e. fish that stayed close to the grid but not on the grid.</li> <li><code>gridtools.preprocessing.remove_poorly_rtacked_tracks</code> to remove tracks that have a low tracking coverage.</li> </ul> <p>Other functions that will be implemented are power-based position estimation of individuals via triangulation.</p>"},{"location":"#simulations","title":"Simulations","text":"<p><code>gridtools</code> provides a toolbox to simulate grid recordings of  multiple communicating fish that move across an electrode grid and can do this based on parameters that are estimated from real recordings.</p> <p>Under construction.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>Contributions are welcome. To develop the package, clone the repository and install it in editable mode. Dev dependencies are managed with <code>poetry</code>. To install them, run:</p> <p><pre><code>poetry install\n</code></pre> Before committing, please run the tests and the linter:</p> <p><pre><code>poetry run black gridtools\npoetry run pylint gridtools\npoetry run pytest\n</code></pre> Only commit if all tests pass and the linter does not report any errors. To make this easier, you can install a pre-commit hook that runs the linter and the tests before every commit:</p> <pre><code>pre-commit install\npre-commit run --all-files\n</code></pre>"},{"location":"#to-do","title":"To do","text":"<ul> <li>[ ] Build unit tests for <code>gridtools.datasets</code></li> <li>[ ] Make a datavis module that provides functions to visualize the data<ul> <li>[ ] Spec, track, pos terminal commands to quickly visualize datasets</li> <li>[ ] animation suite that gets a dataset and start and stop time to animate a full dataset</li> </ul> </li> <li>[ ] Build training data generation pipeline for faster RCNN </li> <li>[x] Test the hybrid grid when data is available</li> <li>[ ] Port the chirp annotation gui from the cnn-chirpdetector to gridtools and rewrite input data handling</li> <li>[ ] Make spectrogram decibel transorm cutoff dynamic for an optimal signal to noise ratio for each 10s window</li> <li>[x] Concerning bounding boxes: Work well for short chirps. I have the impression that either long chirps or chirps with a high curtosis result in too large boxes. I estimate the box width with the standard deviation, maybe a high curtosis results in a large standard deviation. But this does not explain why heigth is a problem as well. The kurtosis could also scale down the amplitude a bit ...</li> <li>[x] The frequency bbox padding needs to be a sum, not a factor, that is basically the freq bin multiplied by a factor to tune it: amp + (freq_bin * factor)</li> <li>[x] Make the extractor run on a folder of datasets instead of individual ones</li> <li>[x] Update the parameter estimation function of the extractor to accomodate the simpler model that is fit to the frequency traces</li> <li>[x] Rewrite hybrid grid function to choose windows where no chirps of the real fish are produced or at least take take snippets that happen during the day when the fish are inactive</li> <li>[ ] Save the first and last spec image of a dataset in the same dimensions as the others but add zero padding.</li> <li>[ ] Add augmentations to the chirps that are simulated from the interpolated parameterspace, i.e. noise, etc. No real chirp is a perfect gaussian, just the average of all chirps are gaussians.</li> <li>[ ] Fix interpolating the parameter space so that the training dataset is uniformly distributed.</li> </ul>"},{"location":"#project-log","title":"Project log","text":"<ul> <li>2023-10-26: Finished datasets module and added documentation.</li> </ul>"},{"location":"datasets/","title":"Datasets","text":""},{"location":"preprocessing/","title":"Preprocessing","text":""},{"location":"simulations/","title":"Simulations","text":""}]}